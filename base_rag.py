# -*- coding: utf-8 -*-
"""IBM_Hackathon.ipynb

Automatically generated by Colab.
"""


import os
#watsonx.ai api keys
project_id = "IBM_PROJECT_ID"
api_key = "WATSONX_API_KEY"

credentials = {
    "url": "https://us-south.ml.cloud.ibm.com",
    "apikey": api_key}

from langchain.vectorstores import FAISS
from langchain_ibm import WatsonxEmbeddings, WatsonxLLM
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenTextParamsMetaNames
import os
import pickle

# define embeddings model
embeddings = WatsonxEmbeddings(
    model_id='ibm/slate-125m-english-rtrvr',
    apikey=credentials.get('apikey'),
    url=credentials.get('url'),
    project_id=project_id
)

# load from disk (if exists) or initialize new FAISS store
faiss_index_path = "faiss_index"

if os.path.exists(faiss_index_path):
    vectorstore = FAISS.load_local(faiss_index_path, embeddings)
else:
    # initialize an empty FAISS store (you'll need to add documents later)
    vectorstore = FAISS.from_texts(["dummy"], embeddings)  # replace with your actual docs
    vectorstore.save_local(faiss_index_path)

from langchain.schema import Document
retriever = vectorstore.as_retriever()

from langchain_community.tools.tavily_search import TavilySearchResults
web_search_tool = TavilySearchResults(tavily_api_key="TAVILY_API_KEY")

llm = WatsonxLLM(
    model_id = "meta-llama/llama-3-405b-instruct",
    url=credentials.get("url"),
    apikey=credentials.get("apikey"),
    project_id=project_id,
    params = {  GenTextParamsMetaNames.DECODING_METHOD: "greedy",
                GenTextParamsMetaNames.MAX_NEW_TOKENS: 1000,
                GenTextParamsMetaNames.TEMPERATURE: 0.7,
                GenTextParamsMetaNames.MIN_NEW_TOKENS: 10})

from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.output_parsers import JsonOutputParser

prompt = PromptTemplate(
    template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are an assistant for environmental product questions, providing comprehensive answers about the environmental impacts of products, including their carbon footprint, water usage, waste generation, and other relevant factors. You should also suggest actionable steps to reduce environmental impact and provide citations for your information.
    {{Below is some context from different sources followed by a user's question. Please answer the question based on the context.

    Documents: {documents}}} <|eot_id|><|start_header_id|>user<|end_header_id|>

    {{ Question: {question} }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

    Answer:
    """,
    input_variables=["question", "documents"],
)

final_prompt = PromptTemplate(
    template="""
<|begin_of_text|><|start_header_id|>system<|end_header_id|>
You are an assistant for environmental product questions, providing comprehensive answers about the environmental impacts of products, including their carbon footprint, water usage, waste generation, and other relevant factors. You should also suggest actionable steps to reduce environmental impact and provide citations for your information.
Given the following context and user question, answer in this JSON format:
{{
  "rating": Number (0-100, representing how confident you are in the accuracy and completeness of your answer based on available data),
  "text": String (comprehensive answer addressing environmental impacts including carbon footprint, water usage, waste generation, etc.),
  "citations": [{{"url": String}}] (list of source URLs that support your answer, minimum 1 source),
  "recommendations": [{{"text": String}}] (2-3 actionable suggestions for reducing environmental impact),
  "suggestedQuestions": [String] (3-4 related follow-up questions users might want to ask)
}}

Tips for each field:
- rating: Consider data quality, source reliability, and how complete the information is
- text: Structure the answer logically, use specific numbers/metrics when available
- citations: Always link to authoritative sources like environmental databases or research papers
- recommendations: Focus on practical, achievable actions for consumers
- suggestedQuestions: Questions should explore related environmental aspects not covered in main answer

Context: {documents}
Question: {question}
<|eot_id|><|start_header_id|>assistant<|end_header_id|>
""",
    input_variables=["question", "documents"],
)

#define rag chain
rag_chain = prompt | llm | StrOutputParser()
# Final RAG chain with the new prompt
final_rag_chain = final_prompt | llm | JsonOutputParser()

prompt = PromptTemplate(
    template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
    You are a grader assessing relevance of a retrieved document to a user question. \n
    Here is the retrieved document: \n\n {document} \n\n

    <|eot_id|><|start_header_id|>user<|end_header_id|>
    Here is the user question: {question} \n

    Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question. \n
    Provide the binary score as a JSON with a single key 'score' and no preamble or explanation. <|eot_id|><|start_header_id|>assistant<|end_header_id|>""",
    input_variables=["question", "document"],
)

#define retieval grader chain
retrieval_grader = prompt | llm | JsonOutputParser()

prompt = PromptTemplate(
    template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
You are a helpful assistant that breaks down user queries about environmental impacts of consumer products into clear sub-questions. 
Your goal is to help the system understand what specific information (e.g., carbon footprint, water usage, recyclability, ethical sourcing) needs to be retrieved to answer the user’s question.

- Focus on aspects such as life cycle assessment (LCA), sustainability, recyclability, emissions, and sourcing practices.
- Always use full product names or descriptions — never use vague pronouns like "it", "they", "these", etc.
- If the question includes a comparison, generate sub-questions for each product.
- If the question is already atomic and needs no further breakdown, respond with: "The question needs no decomposition."

Examples:

Example 1:
Question: What's the carbon footprint of a Nestle chocolate bar compared to an oat-based snack bar?
Decompositions:
What is the carbon footprint of a Nestle chocolate bar?
What is the carbon footprint of an oat-based snack bar?

Example 2:
Question: Is Dove soap recyclable and ethically sourced?
Decompositions:
Is Dove soap recyclable?
Is Dove soap ethically sourced?

Example 3:
Question: Show me the water usage of a T-shirt from H&M.
Decompositions:
What is the water usage of a T-shirt from H&M?

Example 4:
Question: What is the capital of Japan?
Decompositions:
The question needs no decomposition

<|eot_id|><|start_header_id|>user<|end_header_id|>
Question: {user_query} <|eot_id|><|start_header_id|>assistant<|end_header_id|>
Decompositions:""",
    input_variables=["user_query"]
)


#define query decomposition chain
query_decompose = prompt | llm | StrOutputParser()

from typing_extensions import TypedDict, List
from IPython.display import Image, display
from langgraph.graph import START, END, StateGraph

class GraphState(TypedDict):
    """
    Represents the state of our graph.
    Attributes:
        question: question to be used as input in LLM chain
        generation: LLM generation response
        search: "yes" or "no" string acting as boolean for whether to invoke web search
        documents: list of documents for in-context learning
        steps: List of steps taken in agent flow
        user_query: original user query, stored here for persistence during consolidation stage
        sub_answers: list of answers to decomposed questions
    """
    question: str
    generation: str
    search: str
    documents: List[str]
    steps: List[str]
    user_query: str
    sub_answers: List[str]
    sub_questions: List[str]
    final_response: str
    structured_output: dict

def retrieve(state):
    """
    Retrieve documents
    This is the first Node invoked in the CRAG_graph

    # CRAG_graph is invoked in the CRAG_loop node:
    #response = CRAG_graph.invoke({"question": q, "steps": steps})["generation"]
    #we initialize the state with a sub-question and list of steps

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, documents, that contains retrieved documents
    """
    print("---Retrieving Documents---")
    question = state["question"]
    steps = state["steps"]
    steps.append("retrieve_documents")
    documents = retriever.invoke(question)
    return {"documents": documents, "question": question, "steps": steps}

def grade_documents(state):
    """
    Determines whether the retrieved documents are relevant to the question. Store all relevant documents to the documents dictionary.
    However, if there is even one irrelevant document, then websearch will be invoked.

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): Updates documents key with only filtered relevant documents
    """
    print("---Grading Retrieved Documents---")
    documents = state["documents"]
    question = state["question"]
    steps = state["steps"]
    steps.append("grade_document_retrieval")
    relevant_docs = []
    search = "No"
    for d in documents:
        score = retrieval_grader.invoke({"question": question, "document": d.page_content})
        if score["score"] == "yes":
            relevant_docs.append(d)
        else:
            search = "Yes"
    return {"documents": relevant_docs, "question": question, "search": search, "steps": steps}

def decide_to_generate(state):
    """
    Determines whether to generate an answer, or re-generate a question.

    Args:
        state (dict): The current graph state

    Returns:
        str: Binary decision for next node to call
    """
    print("---At decision Edge---")
    """-----------inputs-----------"""
    search = state["search"]

    """-----------actions & outputs-----------"""
    if search == "Yes":
        return "search"
    else:
        return "generate"

def web_search(state):
    print("---Searching the Web---")
    documents = state.get("documents", [])
    question = state["question"]
    steps = state["steps"]
    steps.append("web_search")

    # Add site constraints to bias the search toward trusted domains
    trusted_sites = ["ecoinvent.org", "openlca.org", "unep.org", "sciencebasedtargets.org", "climate-data.org", "ipcc.ch", "world.openfoodfacts.org"]
    constrained_query = question + " " + " OR ".join([f"site:{site}" for site in trusted_sites])

    web_results = web_search_tool.invoke({"query": constrained_query})

    documents.extend(
        [
            Document(page_content=d["content"], metadata={"url": d["url"]})
            for d in web_results
        ]
    )

    return {
        "documents": documents,
        "question": question,
        "steps": steps
    }


def generate(state):
    """
    Generate answer

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, generation, that contains LLM generation
    """
    print("---Generating Response---")
    documents = state["documents"]
    question = state["question"]
    steps = state["steps"]
    steps.append("generating sub-answer")
    query_type = state.get("type", 2)
    if query_type == 1:
        generation = final_rag_chain.invoke({"documents": documents, "question": question})
    else:
        generation = rag_chain.invoke({"documents": documents, "question": question})
    print("Response to subquestion:", generation)
    return {"documents": documents, "question": question, "generation": generation, "steps": steps}

# intialize Graph
CRAG = StateGraph(GraphState)

# === Build CRAG Graph ===
CRAG = StateGraph(GraphState)
CRAG.add_node("retrieve", retrieve)
CRAG.add_node("grade_documents", grade_documents)
CRAG.add_node("generate", generate)
CRAG.add_node("web_search", web_search)
CRAG.set_entry_point("retrieve")
CRAG.add_edge("retrieve", "grade_documents")
CRAG.add_conditional_edges("grade_documents", decide_to_generate, {"search": "web_search", "generate": "generate"})
CRAG.add_edge("web_search", "generate")
CRAG.add_edge("generate", END)
CRAG_graph = CRAG.compile()

# display(Image(CRAG_graph.get_graph(xray=True).draw_mermaid_png()))

def transform_query(state: dict) -> dict:
    """
    Transform the user_query to produce a list of simple questions.
    This is the first node invoked in the graph, with input user question and empty steps list
    response = agentic_rag.invoke({"user_query": question3, "steps": []})

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): Updates question key with a list of re-phrased question
    """
    """-----------inputs-----------"""
    user_query = state["user_query"]
    steps = state["steps"]
    print("User Query:", user_query)
    print("---Decomposing the QUERY---")
    steps.append("transform_query")
    sub_questions = query_decompose.invoke({"user_query": user_query})
    list_of_questions = [q.strip() for q in sub_questions.strip().split('\n')]
    if list_of_questions[0] == 'The question needs no decomposition':
        return {"sub_questions": [user_query], "steps": steps, "user_query": user_query}
    else:
        print("Decomposed into:", list_of_questions)
        return {"sub_questions": list_of_questions, "steps": steps, "user_query": user_query}

def CRAG_loop(state: dict) -> dict:
    questions = state["sub_questions"]
    steps = state["steps"]
    user_query = state["user_query"]
    sub_answers = []
    query_type = state.get("type", 2)
    steps.append("entering iterative CRAG for sub questions")
    for q in questions:
        print("Handling subquestion:", q)
        response = CRAG_graph.invoke({"question": q, "steps": steps, "type": query_type})["generation"]
        sub_answers.append(response)
    return {"sub_answers": sub_answers, "sub_questions": questions, "user_query": user_query, "steps": steps}

def consolidate(state: dict) -> dict:
    print("---Consolidating Response---")
    answers = state['sub_answers']
    questions = state['sub_questions']
    user_query = state['user_query']
    steps = state["steps"]
    steps.append("generating final answer")
    query_type = state.get("type", 2)

    if query_type == 1:
        qa_pairs = [{questions[i]: answers[i]["text"].strip()} for i in range(min(len(questions), len(answers)))]
        final_response = final_rag_chain.invoke({"documents": qa_pairs, "question": user_query})
        print("Final Structured Response:", final_response)
        return {
            "user_query": user_query,
            "final_response": final_response,
            "steps": steps,
            "intermediate_qa": qa_pairs,
        }
    else:
        qa_pairs = [{questions[i]: answers[i].strip()} for i in range(min(len(questions), len(answers)))]
        final_response = rag_chain.invoke({"documents": qa_pairs, "question": user_query})
        print("Final Response to Original Query:", final_response)
        return {
            "user_query": user_query,
            "final_response": final_response,
            "steps": steps,
            "intermediate_qa": qa_pairs,
        }

# === Assemble Nested Graph ===
nested_CRAG = StateGraph(GraphState)
nested_CRAG.add_node("transform_query", transform_query)
nested_CRAG.add_node("CRAG_loop", CRAG_loop)
nested_CRAG.add_node("consolidate", consolidate)
nested_CRAG.set_entry_point("transform_query")
nested_CRAG.add_edge("transform_query", "CRAG_loop")
nested_CRAG.add_edge("CRAG_loop", "consolidate")
nested_CRAG.add_edge("consolidate", END)
# nested_CRAG.add_edge("format_final_output", END)

agentic_rag = nested_CRAG.compile()

# display(Image(agentic_rag.get_graph(xray=True).draw_mermaid_png()))